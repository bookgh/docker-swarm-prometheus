version: '3.3'
services:
  dockerd-exporter:
    image: currycan/dockerd-exporter:v0.0.1
    volumes:
      - /etc/localtime:/etc/localtime:ro
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9323/metrics"]
      interval: 5s
      timeout: 1s
      retries: 3
    networks:
      - monitor
    deploy:
      mode: global
      resources:
        limits:
          memory: 128M
        reservations:
          memory: 64M

  # monitor host
  node-exporter:
    image: currycan/node-exporter:v0.16.0
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
      - /etc/hostname:/etc/nodename:ro
      - /etc/localtime:/etc/localtime:ro
    environment:
      - NODE_ID={{.Node.ID}}
    command:
      - '--path.sysfs=/host/sys'
      - '--path.procfs=/host/proc'
      - '--collector.textfile.directory=/etc/node-exporter/'
      - '--collector.filesystem.ignored-mount-points=^/(sys|proc|dev|host|etc)($$|/)'
      - '--no-collector.ipvs'
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9100/metrics"]
      interval: 5s
      timeout: 1s
      retries: 3
    networks:
      - monitor
    deploy:
      mode: global
      resources:
        limits:
          memory: 128M
        reservations:
          memory: 64M

  # monitor docker container
  cadvisor:
    image: currycan/cadvisor:v0.31.0
    # hostname: '{{.Node.Hostname}}'
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /dev:/dev/:ro
      - /etc/hostname:/etc/nodename:ro
      - /etc/localtime:/etc/localtime:ro
    environment:
      - NODE_ID={{.Node.ID}}
      - NODE_NAME={{.Node.Hostname}}
    command: -logtostderr -docker_only
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/metrics"]
      interval: 5s
      timeout: 1s
      retries: 3
    networks:
      - monitor
    deploy:
      mode: global
      resources:
        limits:
          memory: 128M
        reservations:
          memory: 64M

  # push short-lived jobs
  pushgateway:
    image: currycan/pushgateway:v0.6.0
    volumes:
      - /etc/localtime:/etc/localtime:ro
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9091/metrics"]
      interval: 5s
      timeout: 1s
      retries: 3
    networks:
      - monitor
    # swarm
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role == manager
      resources:
        limits:
          memory: 128M
        reservations:
          memory: 64M

  # datasource
  prometheus:
    image: currycan/prometheus:v2.4.3
    user: root
    volumes:
      - /monitor/prometheus/data:/prometheus
      - /etc/localtime:/etc/localtime:ro
      - /etc/hostname:/etc/nodename:ro
    configs:
      - source: node_rules
        target: /etc/prometheus/swarm_node.rules.yml
      - source: task_rules
        target: /etc/prometheus/swarm_task.rules.yml
    environment:
      - JOBS=pushgateway:9091
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention=24h'
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9090/metrics"]
      interval: 5s
      timeout: 1s
      retries: 3
    networks:
      - monitor
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role == manager
      resources:
        limits:
          memory: 2048M
        reservations:
          memory: 128M

  # alert
  alertmanager:
    image: currycan/alertmanager:v0.15.2
    volumes: 
      - /etc/localtime:/etc/localtime:ro
    configs:
      - source: alert_config
        target: /etc/prometheus/alertmanager.yml
    environment:
      - SLACK_URL=${SLACK_URL:-https://hooks.slack.com/services/TOKEN}
      - SLACK_CHANNEL=${SLACK_CHANNEL:-general}
      - SLACK_USER=${SLACK_USER:-alertmanager}
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      - '--storage.path=/alertmanager'
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9093/"]
      interval: 5s
      timeout: 1s
      retries: 3
    networks:
      - monitor
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role == manager
      resources:
        limits:
          memory: 128M
        reservations:
          memory: 64M

  unsee:
    image: currycan/unsee:v0.9.2
    environment:
      - "ALERTMANAGER_URIS=default:http://alertmanager:9093"
      - "PORT=9080"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9080"]
      interval: 5s
      timeout: 1s
      retries: 3
    networks:
      - monitor
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role == manager
      resources:
        limits:
          memory: 128M
        reservations:
          memory: 64M

  # dashboard
  grafana:
    image: currycan/grafana:v5.3.1
    volumes:
      - /monitor/grafana/data:/grafana/data
      - /etc/localtime:/etc/localtime:ro
    environment:
      - GF_SECURITY_ADMIN_USER=${ADMIN_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD=${ADMIN_PASSWORD:-admin}
      - GF_USERS_ALLOW_SIGN_UP=false
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/"]
      interval: 5s
      timeout: 1s
      retries: 3
    networks:
      - monitor
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role == manager
      resources:
        limits:
          memory: 128M
        reservations:
          memory: 64M

  # web authorization
  caddy:
    image: currycan/caddy:v0.11.0
    ports:
      # grafana
      - "23000:3000"
      # prometheus
      - "29090:9090"
      # alertmanager
      - "29093:9093"
      # nodeexporter
      - "29100:9100"
      # cadvisor
      - "28080:8080"
      # unsee
      - "29098:9080"
      # pushgateway
      - "29091:9091"
      # dockerd-exporter
      - "29323:9323"
    volumes:
      - /etc/localtime:/etc/localtime:ro
    configs:
      - source: caddy_config
        target: /etc/caddy/Caddyfile
    environment:
      - ADMIN_USER=${ADMIN_USER:-admin}
      - ADMIN_PASSWORD=${ADMIN_PASSWORD:-admin}
    networks:
      - monitor
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role == manager
      resources:
        limits:
          memory: 128M
        reservations:
          memory: 64M
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000"]
      interval: 5s
      timeout: 1s
      retries: 5

networks:
  monitor:
    driver: overlay
    attachable: true

configs:
  caddy_config:
    file: ./caddy/config/Caddyfile
  node_rules:
    file: ./prometheus/rules/swarm_node.rules.yml
  task_rules:
    file: ./prometheus/rules/swarm_task.rules.yml
  alert_config:
    file: ./alertmanager/config/alertmanager.yml